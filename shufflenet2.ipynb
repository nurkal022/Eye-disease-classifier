{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqZ7F-q5A6lc"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJs2QCkA9umU"
   },
   "outputs": [],
   "source": [
    "def channel_shuffle(x, groups=2):\n",
    "  bat_size, channels, w, h = x.shape\n",
    "  group_c = channels // groups\n",
    "  x = x.view(bat_size, groups, group_c, w, h)\n",
    "  x = t.transpose(x, 1, 2).contiguous()\n",
    "  x = x.view(bat_size, -1, w, h)\n",
    "  return x\n",
    "\n",
    "# used in the block\n",
    "def conv_1x1_bn(in_c, out_c, stride=1):\n",
    "  return nn.Sequential(\n",
    "    nn.Conv2d(in_c, out_c, 1, stride, 0, bias=False),\n",
    "    nn.BatchNorm2d(out_c),\n",
    "    nn.ReLU(True)\n",
    "  )\n",
    "\n",
    "def conv_bn(in_c, out_c, stride=2):\n",
    "  return nn.Sequential(\n",
    "    nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False),\n",
    "    nn.BatchNorm2d(out_c),\n",
    "    nn.ReLU(True)\n",
    "  )\n",
    "\n",
    "\n",
    "class ShuffleBlock(nn.Module):\n",
    "  def __init__(self, in_c, out_c, downsample=False):\n",
    "    super(ShuffleBlock, self).__init__()\n",
    "    self.downsample = downsample\n",
    "    half_c = out_c // 2\n",
    "    if downsample:\n",
    "      self.branch1 = nn.Sequential(\n",
    "          # 3*3 dw conv, stride = 2\n",
    "          nn.Conv2d(in_c, in_c, 3, 2, 1, groups=in_c, bias=False),\n",
    "          nn.BatchNorm2d(in_c),\n",
    "          # 1*1 pw conv\n",
    "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          nn.ReLU(True)\n",
    "      )\n",
    "      \n",
    "      self.branch2 = nn.Sequential(\n",
    "          # 1*1 pw conv\n",
    "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          nn.ReLU(True),\n",
    "          # 3*3 dw conv, stride = 2\n",
    "          nn.Conv2d(half_c, half_c, 3, 2, 1, groups=half_c, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          # 1*1 pw conv\n",
    "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          nn.ReLU(True)\n",
    "      )\n",
    "    else:\n",
    "      # in_c = out_c\n",
    "      assert in_c == out_c\n",
    "        \n",
    "      self.branch2 = nn.Sequential(\n",
    "          # 1*1 pw conv\n",
    "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          nn.ReLU(True),\n",
    "          # 3*3 dw conv, stride = 1\n",
    "          nn.Conv2d(half_c, half_c, 3, 1, 1, groups=half_c, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          # 1*1 pw conv\n",
    "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
    "          nn.BatchNorm2d(half_c),\n",
    "          nn.ReLU(True)\n",
    "      )\n",
    "      \n",
    "      \n",
    "  def forward(self, x):\n",
    "    out = None\n",
    "    if self.downsample:\n",
    "      # if it is downsampling, we don't need to do channel split\n",
    "      out = t.cat((self.branch1(x), self.branch2(x)), 1)\n",
    "    else:\n",
    "      # channel split\n",
    "      channels = x.shape[1]\n",
    "      c = channels // 2\n",
    "      x1 = x[:, :c, :, :]\n",
    "      x2 = x[:, c:, :, :]\n",
    "      out = t.cat((x1, self.branch2(x2)), 1)\n",
    "    return channel_shuffle(out, 2)\n",
    "    \n",
    "\n",
    "class ShuffleNet2(nn.Module):\n",
    "  def __init__(self, num_classes=2, input_size=224, net_type=1):\n",
    "    super(ShuffleNet2, self).__init__()\n",
    "    assert input_size % 32 == 0 # 因为一共会下采样32倍\n",
    "    \n",
    "    \n",
    "    self.stage_repeat_num = [4, 8, 4]\n",
    "    if net_type == 0.5:\n",
    "      self.out_channels = [3, 24, 48, 96, 192, 1024]\n",
    "    elif net_type == 1:\n",
    "      self.out_channels = [3, 24, 116, 232, 464, 1024]\n",
    "    elif net_type == 1.5:\n",
    "      self.out_channels = [3, 24, 176, 352, 704, 1024]\n",
    "    elif net_type == 2:\n",
    "      self.out_channels = [3, 24, 244, 488, 976, 2948]\n",
    "    else:\n",
    "      print(\"the type is error, you should choose 0.5, 1, 1.5 or 2\")\n",
    "      \n",
    "    # let's start building layers\n",
    "    self.conv1 = nn.Conv2d(3, self.out_channels[1], 3, 2, 1)\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    in_c = self.out_channels[1]\n",
    "    \n",
    "    self.stages = []\n",
    "    for stage_idx in range(len(self.stage_repeat_num)):\n",
    "      out_c = self.out_channels[2+stage_idx]\n",
    "      repeat_num = self.stage_repeat_num[stage_idx]\n",
    "      for i in range(repeat_num):\n",
    "        if i == 0:\n",
    "          self.stages.append(ShuffleBlock(in_c, out_c, downsample=True))\n",
    "        else:\n",
    "          self.stages.append(ShuffleBlock(in_c, in_c, downsample=False))\n",
    "        in_c = out_c\n",
    "    self.stages = nn.Sequential(*self.stages)\n",
    "    \n",
    "    in_c = self.out_channels[-2]\n",
    "    out_c = self.out_channels[-1]\n",
    "    self.conv5 = conv_1x1_bn(in_c, out_c, 1)\n",
    "    self.g_avg_pool = nn.AvgPool2d(kernel_size=(int)(input_size/32)) # 如果输入的是224，则此处为7\n",
    "    \n",
    "    # fc layer\n",
    "    self.fc = nn.Linear(out_c, num_classes)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = self.stages(x)\n",
    "    x = self.conv5(x)\n",
    "    x = self.g_avg_pool(x)\n",
    "    x = x.view(-1, self.out_channels[-1])\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3FNodcmwtS3"
   },
   "outputs": [],
   "source": [
    "model = ShuffleNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEAIPfRpKQyA"
   },
   "outputs": [],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1i1wZ7QNMR_"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Пути к папкам с данными\n",
    "train_data_path = \"model/train\"\n",
    "test_data_path = \"model/test\"\n",
    "\n",
    "# Преобразования изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Размер, подходящий для вашей модели\n",
    "    transforms.ToTensor(),           # Преобразование изображений в тензоры\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация данных\n",
    "])\n",
    "\n",
    "# Создание датасетов\n",
    "train_dataset = ImageFolder(root=train_data_path, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_data_path, transform=transform)\n",
    "\n",
    "# Создание загрузчиков данных\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBlZLgcyPTGd"
   },
   "outputs": [],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "model = ShuffleNet2()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_YYFannPi3Z"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rr_xnFcN0WV"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, loss_fn, optimizer, num_epochs=5):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.\n",
    "    val_acc_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0.\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                with t.autograd.set_grad_enabled(phase==\"train\"):\n",
    "                    outputs = model(inputs) # bsize * 2 , because it is a binary classification\n",
    "                    loss = loss_fn(outputs, labels) \n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                if phase == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += t.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(\"Phase {} loss: {}, acc: {}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"val\":\n",
    "                val_acc_history.append(epoch_acc)\n",
    "    model.load_state_dict(best_model_wts)    \n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASB6f6Ayabbo"
   },
   "outputs": [],
   "source": [
    "model, val_logs = train_model(model, dataloader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.save(model.state_dict(), \"./save/\" + str(int(time.time()))+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Пути к папкам с данными\n",
    "train_data_path = \"model/train\"\n",
    "test_data_path = \"model/test\"\n",
    "\n",
    "# Преобразования изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Размер, подходящий для вашей модели\n",
    "    transforms.ToTensor(),           # Преобразование изображений в тензоры\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация данных\n",
    "])\n",
    "\n",
    "# Создание датасетов\n",
    "train_dataset = ImageFolder(root=train_data_path, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_data_path, transform=transform)\n",
    "\n",
    "# Создание загрузчиков данных\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устройство для обучения (GPU если доступен, иначе CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Создание модели и перенос на устройство\n",
    "model = ShuffleNet2()\n",
    "model = model.to(device)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Функция обучения модели\n",
    "def train_model(model, dataloaders, loss_fn, optimizer, num_epochs=5):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.\n",
    "    val_acc_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0.\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                with torch.autograd.set_grad_enabled(phase==\"train\"):\n",
    "                    outputs = model(inputs) # bsize * 2 , because it is a binary classification\n",
    "                    loss = loss_fn(outputs, labels) \n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                if phase == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(\"Phase {} loss: {:.4f}, acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"val\":\n",
    "                val_acc_history.append(epoch_acc)\n",
    "    model.load_state_dict(best_model_wts)    \n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6264\\1874690397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Обучение модели\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Сохранение лучшей модели\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6264\\2359808407.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# bsize * 2 , because it is a binary classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3059\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Создание словаря для загрузчиков данных\n",
    "dataloaders = {\"train\": train_loader, \"val\": test_loader}\n",
    "\n",
    "# Обучение модели\n",
    "model, val_logs = train_model(model, dataloaders, loss_fn, optimizer)\n",
    "\n",
    "# Сохранение лучшей модели\n",
    "torch.save(model.state_dict(), \"./\" + str(int(time.time())) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "shufflenet2",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
